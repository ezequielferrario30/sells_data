# ğŸ“ Proyecto Integrador DE â€“ README

---

## 1. Â¿QuÃ© hice?

Se diseÃ±Ã³ e implementÃ³ un sistema completo de anÃ¡lisis de ventas para una startup con varias sucursales.
El objetivo es implementar un sistema que procese archivos de ventas generados diariamente, organice la informacion en una base de daos y permita  realizar analisis.

Este sistema abarca desde la carga de datos desde archivos CSV a una base de datos MySQL, el modelado orientado a objetos en Python, la aplicaciÃ³n de patrones de diseÃ±o (Factory, Singleton), la construcciÃ³n de objetos SQL avanzados (funciÃ³n, trigger, procedimiento almacenado, vista, Ã­ndice), la ejecuciÃ³n de consultas SQL avanzadas (CTEs y funciones de ventana), la integraciÃ³n total desde Python, la visualizaciÃ³n analÃ­tica, y la validaciÃ³n con testing automatizado y cobertura profesional.

---

## 2. Â¿CÃ³mo estÃ¡ organizado el proyecto?

```
ventas_data_engineering/
â”œâ”€â”€ venv/                   # Entorno virtual (EXCLUIDO del repositorio)
â”œâ”€â”€ data/                   # CSVs originales (input)
â”‚   â”œâ”€â”€ categories.csv
â”‚   â”œâ”€â”€ cities.csv
â”‚   â”œâ”€â”€ countries.csv
â”‚   â”œâ”€â”€ customers.csv
â”‚   â”œâ”€â”€ employees.csv
â”‚   â”œâ”€â”€ products.csv
â”‚   â””â”€â”€ sales.csv
â”‚
â”œâ”€â”€ src/                    # CÃ³digo fuente principal del proyecto
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ database.py
â”‚   â”œâ”€â”€ database_sqlalchemy.py
â”‚   â”œâ”€â”€ load_data.py
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ category.py
â”‚       â”œâ”€â”€ city.py
â”‚       â”œâ”€â”€ country.py
â”‚       â”œâ”€â”€ customer.py
â”‚       â”œâ”€â”€ employee.py
â”‚       â”œâ”€â”€ product.py
â”‚       â”œâ”€â”€ sale.py
â”‚       â””â”€â”€ factory.py
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ load_data.sql
â”‚   â”œâ”€â”€ create_functions.sql
â”‚   â”œâ”€â”€ create_views.sql
â”‚   â”œâ”€â”€ create_procedures.sql
â”‚   â”œâ”€â”€ create_triggers.sql
â”‚   â”œâ”€â”€ create_indexes.sql
â”‚   â””â”€â”€ analysis_queries.sql
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_customer.py
â”‚   â”œâ”€â”€ test_product.py
â”‚   â”œâ”€â”€ test_sale.py
â”‚   â”œâ”€â”€ test_factory.py
â”‚   â”œâ”€â”€ test_database_sqlalchemy.py
â”‚   â”œâ”€â”€ test_integracion_objetos_sql.py
â”‚   
â”‚
â”œâ”€â”€ integracion_final.ipynb         # Notebook de integraciÃ³n Avance 1 (base, modelos, queries, patrones, tests)
â”œâ”€â”€ integracion_objetos_sql.ipynb   # Notebook de integraciÃ³n Avance 2 (objetos SQL, ejecuciÃ³n, visualizaciones)
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```



---

## 3. JustificaciÃ³n tÃ©cnica y decisiones clave

### Carga de datos automatizada

- **Script**: `load_data.sql` con `LOAD DATA LOCAL INFILE` para importar eficientemente los CSV.
- **Ventajas**: Recarga grandes volÃºmenes, reproducible, portable y trazable. Evita errores manuales.

### Modelado orientado a objetos (POO)

- **Clases**: Cada entidad de negocio (producto, cliente, venta, etc.) es una clase Python con encapsulamiento, constructores y mÃ©todos de negocio.
- **Ventajas**: Centraliza las reglas, facilita validaciones y cambios futuros del modelo.

### Patrones de diseÃ±o: comparaciÃ³n y justificaciÃ³n

| PatrÃ³n         | UbicaciÃ³n / AplicaciÃ³n                 | Â¿Para quÃ© se usa?                                        | Ventajas principales                                                                      | Â¿Por quÃ© se eligiÃ³ sobre otros?                          |
| -------------- | -------------------------------------- | -------------------------------------------------------- | ----------------------------------------------------------------------------------------- | -------------------------------------------------------- |
| Factory Method | src/models/factory.py                  | Centralizar y estandarizar creaciÃ³n de entidades         | Modularidad, escalabilidad, mantenimiento, nuevas fuentes de datos fÃ¡ciles                | MÃ¡s simple/flexible que  Factory o Builder        |
| Singleton      | src/database.py, src/database_sqlalchemy.py | Garantizar instancia Ãºnica de conexiÃ³n a la base de datos | Eficiencia, seguridad, evita fugas de recursos, punto de acceso Ãºnico                     | MÃ¡s simple/directo que Object Pool o Service Locator      |

Factory me permite tener un solo lugar en el cÃ³digo donde se crean los objetos de mi negocio: productos, clientes, ventas, etc.
Si maÃ±ana cambia la forma en que se crea un cliente (por ejemplo, agrego un campo, o cambio el tipo de dato), solo modifico el mÃ©todo Factory, no tengo que buscar en todo el cÃ³digo dÃ³nde instancio clientes.

TambiÃ©n me sirve para automatizar la creaciÃ³n de objetos a partir de datos (como dicts de un CSV).
Por ejemplo, puedo leer una fila del CSV y decirle a la Factory: â€œcreame un producto con estos datosâ€ y listo, sin repetir lÃ³gica.

AdemÃ¡s, si algÃºn dÃ­a tengo diferentes versiones de una entidad (por ejemplo, â€œClienteVIPâ€ o â€œProductoPremiumâ€), la Factory puede decidir a quiÃ©n instanciar segÃºn el contexto.

Use este metodo para tener el cÃ³digo centralizado, ordenado y fÃ¡cil de mantener, siguiendo buenas prÃ¡cticas de diseÃ±o. Me parece mucho mejor utilizar este patron porque es mucho mas simple y eficiente para lo que tenia que hacer, al no ser objetos muy complejos  me parecio mejor que Builder y tampoco tengo familia entera de objetos como para usar Abstact Factory.


La parte de las conexiones de datos creo que es una de las mas importantes:


- Si abro muchas conexiones a la vez, puedo sobrecargar la base y hacer lento el sistema.

- Si cada parte del programa crea su propia conexiÃ³n, se complica la gestiÃ³n y debugging.

Singleton garantiza que solo haya una Ãºnica conexiÃ³n  en todo el sistema.
Cuando cualquier parte del cÃ³digo necesita conectarse, simplemente usa esa conexiÃ³n ya creada.

Esto hace el cÃ³digo mÃ¡s eficiente, seguro y fÃ¡cil de testear, porque si necesito cambiar algo de la conexiÃ³n (por ejemplo, la base de datos, usuario, etc.), lo hago en un solo lugar y se aplica en toda la aplicaciÃ³n.

Usar este metodo me permiteque la conexiÃ³n sea Ãºnica y controlada, evitando errores y mejorando el rendimiento del sistema. Si bien hay otros metodos, en este caso solo tenia que cargar csv, es decir, datos de un mismo tipo por lo que me parecio lo mas simple y eficiente para no elegir otro patron.


----

### Escalabilidad

El foco en la escalabilidad estuvo puesto todo el tiempo.

Es por ello que se le dio vital importancia a la modularizacion:

- Separacion por carpetas de forma clara (Data para la fuente de datos, src para el codigo en python, sql para los scripts de SQL, test para los tets)

- Cada entidad de negocio tiene su propio archivo para facilitar los cambios, a s u vez toda la logica de conexion a la base de datos estan en modulos separados como tambien los funciones y procedimientos en SQL

- Se aplicaron patrones de diseÃ±o como Factory Method y Singleton

De esta manera el codigo es mucho mas facil de entender, mantener y ampliar ya que  al tener todo modulado por parte varias personas o equipos pueden trabajar al mismo tiempo.


---
### Objetos SQL y consultas avanzadas

- **FunciÃ³n**: `calcular_descuento` centraliza la lÃ³gica de descuentos por tipo de cliente.
- **Vista**: `ventas_mensuales_ciudad_categoria` para reportes segmentados y dashboards rÃ¡pidos.
- **Procedimiento almacenado**: `registrar_venta` estandariza la carga de ventas, permite validar y auditar.
- **Trigger**: registra logs de ventas en tiempo real y aplica reglas automÃ¡ticas, asegurando trazabilidad.
- **Ãndice**: mejora la performance en consultas frecuentes sobre ventas.
- **Consultas avanzadas**: uso intensivo de CTEs y funciones de ventana (`RANK`, `ROW_NUMBER`, `DENSE_RANK`, `SUM() OVER`).

**DecisiÃ³n**:  
El diseÃ±o de estos objetos responde a necesidades analÃ­ticas y de negocio reales, y demuestra cÃ³mo el modelo puede evolucionar con nuevas reglas.

---

### IntegraciÃ³n total desde Python

- Todo el pipeline (creaciÃ³n y consulta de objetos SQL, ejecuciÃ³n de funciones/procedimientos, visualizaciones, logs) se realiza desde Python usando SQLAlchemy y pandas en conjunto con otras librerias.


---

### Visualizaciones analÃ­ticas

- Incluye grÃ¡ficos de:  
    - EvoluciÃ³n de ventas mensuales  
    - Top productos y clientes  
    - DistribuciÃ³n por ciudad y tipo de cliente  
    - EvoluciÃ³n por categorÃ­a  
- Las visualizaciones apoyan la interpretaciÃ³n de los resultados y la toma de decisiones estratÃ©gicas.

---

### Testing y calidad

- Tests unitarios y de integraciÃ³n con `pytest`:
    - Validan modelos, lÃ³gica de negocio, patrones y objetos SQL.
    - Chequean que los triggers y funciones trabajan en conjunto y que la integraciÃ³n Python-SQL es correcta.
- Asegura robustez, calidad, y fÃ¡cil refactorizaciÃ³n.

---

### Seguridad y buenas prÃ¡cticas

- Credenciales sÃ³lo en `.env` (no versionado).
- CÃ³digo seguro, portable y fÃ¡cil de configurar.
- `.gitignore` bien configurado, sin datos sensibles ni entorno virtual.

---

### OrganizaciÃ³n y reproducibilidad

- Proyecto ordenado por responsabilidad, versiÃ³n controlada y documentada.
- Notebooks integradores (`integracion_final.ipynb`, `integracion_objetos_sql.ipynb`) muestran outputs, explicaciones y justificaciones. En el primero se hizo la parte de P2 y en el segundo el de P3.

---

## 4. Â¿CÃ³mo ejecutar el proyecto?

**1. Clonar el repositorio y crear un entorno virtual:**
```bash
git clone [URL_DEL_REPO]
cd ventas_data_engineering
python -m venv venv
source venv/bin/activate  # O .\venv\Scripts\activate en Windows
pip install -r requirements.txt
```

2. Configurar el archivo .env con credenciales de la base.

3. Ejecutar el script SQL de carga:

Desde MySQL Workbench: abrir y ejecutar sql/load_data.sql y los demÃ¡s scripts de sql/ para crear funciones, triggers, vistas, Ã­ndices y procedimientos.

4. Correr los tests:

```

pytest


```

5. Ejecutar los notebooks:

integracion_final.ipynb (avance 1, integraciÃ³n base y modelos)

integracion_objetos_sql.ipynb (avance 2, objetos SQL, ejecuciones, outputs y visualizaciones)

6. Explorar o ejecutar scripts adicionales desde src/.

----- 

# 5. Repositorio y versionado

- Todo el cÃ³digo, SQL, tests, notebooks y documentaciÃ³n estÃ¡n versionados y justificados.

- No  hay datos sensibles ni entorno virtual subido, cumpliendo estÃ¡ndares de seguridad y profesionalismo.

- Cada cambio estÃ¡ registrado para trazabilidad y auditorÃ­a.

---

## Lecciones aprendidas

- La importancia de la **modularidad y el versionado**: Organizar el cÃ³digo y los .sql en carpetas bien definidas hace que sea mas comodo y por ende mas facil el desarrollo y el debugging.
- **Automatizar la carga y modelado** desde el inicio limita errores y permite iterar mÃ¡s rÃ¡pido sobre los datos.
- La **integraciÃ³n entre Python y SQL** permite construir pipelines reproducibles y profesionales, facilitando el anÃ¡lisis y la visualizaciÃ³n de los resultados, para luego integrar con librerias como pandas.
- El uso de **patrones de diseÃ±o**  hacen el sistema mucho mÃ¡s mantenible, especialmente cuando los requerimientos cambian.
- Los **tests de integraciÃ³n** ayudan a detectar rÃ¡pidamente cualquier ruptura en la lÃ³gica entre objetos SQL y cÃ³digo Python, ahorrando tiempo en debugging.
- Documentar **decisiones y supuestos** en README y notebooks hace que el trabajo sea entendible y defendible ante cualquier proceso de revisado




# 6. ReflexiÃ³n final

Cada decisiÃ³n de diseÃ±o se fundamentÃ³ en maximizar mantenibilidad, eficiencia, escalabilidad y claridad, usando patrones y tÃ©cnicas seleccionadas por sus ventajas reales y justificadas en cada punto.
La soluciÃ³n refleja tanto la aplicaciÃ³n del conocimiento tÃ©cnico como la capacidad de razonar y justificar decisiones, habilidades clave para un ingeniero de datos profesional.

El resultado es un sistema reproducible, robusto, seguro y defendible, alineado con las mejores prÃ¡cticas del sector y las expectativas del negocio.